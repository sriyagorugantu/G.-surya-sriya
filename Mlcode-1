import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Evaluation
from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, classification_report
)

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
data=pd.read_excel("C:/Users/DHANYA/Downloads/Assignment PS6/defect_dataset-ant-1.3.xlsx")
data
data.head(2)
plt.figure()
sns.countplot(x=data.iloc[:, -1])
plt.title("Target Class Distribution")
plt.show()
data.hist(figsize=(15, 10))
plt.show()
numeric_df = data.select_dtypes(include=['int64', 'float64'])
correlation_matrix =numeric_df.corr(method='pearson')

# Display correlation matrix
print(correlation_matrix)

# Plot correlation heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(
    correlation_matrix,
    cmap="coolwarm",
    annot=False,
    linewidths=0.5
)

plt.title("Correlation Heatmap of Software Defect Dataset")
plt.tight_layout()
plt.show()
data.isnull().sum()
data.isnull().values.any()
numeric_df = data.select_dtypes(include=np.number)
numeric_df.skew().sort_values(ascending=False)
from sklearn.preprocessing import StandardScaler

# Separate features and target
X = data.drop(columns=data.columns[-1])   # Features
y = data[data.columns[-1]]                 # Target variable

# Select only numerical features (important for safety)
X_numeric = X.select_dtypes(include=['int64', 'float64'])

# Apply Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# Convert back to DataFrame (very important for readability & further processing)
X_scaled = pd.DataFrame(
    X_scaled,
    columns=X_numeric.columns,
    index=X_numeric.index
)

X_scaled.head()
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


X = data.drop(columns=data.columns[-1])   # Features
y = data[data.columns[-1]]                 # Target

# Select only numerical features
X = X.select_dtypes(include=['int64', 'float64'])


X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)


scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


X_train_scaled = pd.DataFrame(
    X_train_scaled,
    columns=X_train.columns,
    index=X_train.index
)

X_test_scaled = pd.DataFrame(
    X_test_scaled,
    columns=X_test.columns,
    index=X_test.index
)


X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X,
    y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# Scale second split
X_train2_scaled = scaler.fit_transform(X_train2)
X_test2_scaled = scaler.transform(X_test2)

X_train2_scaled = pd.DataFrame(
    X_train2_scaled,
    columns=X_train2.columns,
    index=X_train2.index
)

X_test2_scaled = pd.DataFrame(
    X_test2_scaled,
    columns=X_test2.columns,
    index=X_test2.index
)
split_summary = pd.DataFrame({
    "Dataset": ["X_train", "X_test", "y_train", "y_test"],
    "Shape": [
        X_train_scaled.shape,
        X_test_scaled.shape,
        y_train.shape,
        y_test.shape
    ]
})

split_summary
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
y_prob_lr = lr.predict_proba(X_test)[:, 1]
print(y_pred_lr)
print(y_prob_lr)
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
y_prob_dt = dt.predict_proba(X_test)[:, 1]
print(y_pred_dt)
print(y_prob_dt)
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(
    dt,
    feature_names=X_train.columns,
    class_names=["No Defect", "Defect"],
    filled=True,
    rounded=True,
    fontsize=8
)
plt.title("Decision Tree Diagram")
plt.show()
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]
print(y_pred_rf)
print(y_prob_rf)
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Select one tree from the forest
single_tree = rf.estimators_[0]

plt.figure(figsize=(20, 10))
plot_tree(
    single_tree,
    feature_names=X_train.columns,
    class_names=["No Defect", "Defect"],
    filled=True,
    rounded=True,
    fontsize=8
)
plt.title("Diagram of One Decision Tree from Random Forest")
plt.show()
models = {
    "Logistic Regression": (y_test, y_pred_lr, y_prob_lr),
    "Decision Tree": (y_test, y_pred_dt, y_prob_dt),
    "Random Forest": (y_test, y_pred_rf, y_prob_rf)
}

results = []

for name, (yt, yp, yprob) in models.items():
    results.append({
        "Model": name,
        "Precision": precision_score(yt, yp),
        "Recall": recall_score(yt, yp),
        "F1-Score": f1_score(yt, yp),
        "ROC-AUC": roc_auc_score(yt, yprob)
    })

results_df = pd.DataFrame(results)
results_df
results_df.set_index("Model").plot(kind="bar", figsize=(10, 6))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.show()
